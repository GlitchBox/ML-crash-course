1)Training error should steadily decrease, steeply at first, and should eventually plateau as training converges.

2)If the training has not converged, try running it for longer.

3)If the training error decreases too slowly, increasing the learning rate may help it decrease faster.
But sometimes the exact opposite may happen if the learning rate is too high.

4)If the training error varies wildly, try decreasing the learning rate.

5)Lower learning rate plus larger number of steps or larger batch size is often a good combination.

6)Very small batch sizes can also cause instability. First try larger values like 100 or 1000, and decrease until you see degradation.